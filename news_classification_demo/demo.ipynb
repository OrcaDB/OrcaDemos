{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup/Config\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependencies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'EXACT_MATCH_THRESHOLD' from 'orca_common' (/Users/ryankopec/Library/Caches/pypoetry/virtualenvs/orcademo-Mh9Exlzv-py3.11/lib/python3.11/site-packages/orca_common/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Optional, List\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01morcalib\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01morca\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01morca_common\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TableCreateMode\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01morcalib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01morca_torch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ClassificationMode, OrcaModel\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/orcademo-Mh9Exlzv-py3.11/lib/python3.11/site-packages/orcalib/__init__.py:7\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Any, Optional\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataFrame\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01morca_common\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      8\u001b[0m     EXACT_MATCH_THRESHOLD,\n\u001b[1;32m      9\u001b[0m     CatchupStatus,\n\u001b[1;32m     10\u001b[0m     ColumnName,\n\u001b[1;32m     11\u001b[0m     EmbeddingModel,\n\u001b[1;32m     12\u001b[0m     ImageFormat,\n\u001b[1;32m     13\u001b[0m     Order,\n\u001b[1;32m     14\u001b[0m     RowDict,\n\u001b[1;32m     15\u001b[0m     TableCreateMode,\n\u001b[1;32m     16\u001b[0m )\n\u001b[1;32m     18\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTableCreateMode\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mImageFormat\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRowDict\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     27\u001b[0m ]\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01morcalib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatabase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _with_default_database_method\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'EXACT_MATCH_THRESHOLD' from 'orca_common' (/Users/ryankopec/Library/Caches/pypoetry/virtualenvs/orcademo-Mh9Exlzv-py3.11/lib/python3.11/site-packages/orca_common/__init__.py)"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "import gradio as gr\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from typing import Optional, List\n",
    "import os\n",
    "\n",
    "import orcalib as orca\n",
    "from orcalib.orca_torch import ClassificationMode, OrcaModel\n",
    "from orcalib.orca_classification import OrcaClassificationHead\n",
    "from orcalib.orca_torch_mixins import LabelColumnNameMixin, DropExactMatchOption"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Config\n",
    "\n",
    "If you are training a new model, please update the following config values:\n",
    "\n",
    "```\n",
    "USE_PRETRAINED = False\n",
    "DO_TRAIN = True\n",
    "```\n",
    "\n",
    "If you prefer to use a pre-trained model, you can find one for this notebook [here](https://drive.google.com/drive/folders/1SKcZpeVPyG-Mo216RP8d6BNT7Cbon3oP).\n",
    "\n",
    "If you are starting with a clean DB, update the following config value:\n",
    "\n",
    "```\n",
    "RELOAD_MEMORY_TABLE = True\n",
    "```\n",
    "\n",
    "Note: that this will take a little bit of time, but you will see a progress bar.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global Config\n",
    "\n",
    "DEVICE = \"mps\"  # 'mps' (mac), 'cpu', or 'cuda'\n",
    "DTYPE = torch.float32\n",
    "NUM_MEMS = 10  # how many memories should the model use\n",
    "USE_PRETRAINED = False  # should the pre-trained model be loaded (independent of DO_TRAIN)\n",
    "PRETRAINED_PATH = \"news_classification_demo_model.pt\"  # path to save/load the model\n",
    "\n",
    "SHRINK_DATASET = True  # should the dataset be shrunk to a smaller size for rapid testing\n",
    "\n",
    "# Data Loading Config\n",
    "RELOAD_MEMORY_TABLE = True\n",
    "\n",
    "# Training Config\n",
    "BATCH_SIZE = 32\n",
    "NUM_EPOCHS = 1  # how many epochs to train for (if training is enabled)\n",
    "LEARNING_RATE = 1e-4\n",
    "DO_TRAIN = True  # should the model be trained (leave as False to use the pretrained model without tuning)\n",
    "DO_SAVE = True  # should the model be saved after training (WARNING: will ovewrite PRETRAINED_PATH if True)\n",
    "REBUILD_DATASET_LOOKUPS = True  # When True, the dataset lookups will be rebuilt. Otherwise, they will be loaded from disk.\n",
    "\n",
    "# Testing Config\n",
    "DO_START_UI = False  # simple gradio UI to manually test the model\n",
    "DO_RUN_BENCHMARK = True  # benchmark the model on the test set and record results with Orca\n",
    "\n",
    "# Names and Paths\n",
    "MEMORY_TABLE_NAME = \"memory_table\"\n",
    "MEMORY_INDEX_NAME = \"memory_index\"\n",
    "ORCA_DATABASE_NAME = \"news_classification_demo\"\n",
    "\n",
    "TRAIN_DATASET_FILENAME = os.path.join(\"data\", \"trainset_cached\")\n",
    "TEST_DATASET_FILENAME = os.path.join(\"data\", \"testset_cached\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're using a relatively simple open source dataset here that consists of ~120k news headlines, categorized into 4 categories: (World News, Sports, Business, Science/Technology). It has the advantage of being small enough to allow for quick iteration and experimentation, but it's not so small as to be unrealistic compared to real world datasets. It also contains quite a few mis-classifications in the training data, making for a good \"real world data quality\" approximation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = datasets.load_dataset(\"ag_news\")  # vanilla hugingface datasets here\n",
    "ds_test = ds[\"test\"]\n",
    "ds_train = ds[\"train\"]\n",
    "print(\"Loaded ag_news dataset\")\n",
    "\n",
    "if SHRINK_DATASET:\n",
    "    # Select a subset of the training/testing data to use as memories\n",
    "    # This is useful for rapid testing\n",
    "    ds_split = ds_train.train_test_split(test_size=0.05)\n",
    "    ds_train = ds_split[\"test\"]\n",
    "    print(\"\\tShrank training dataset to 5% of original size\")\n",
    "\n",
    "    ds_split = ds_test.train_test_split(test_size=0.5)\n",
    "    ds_test = ds_split[\"test\"]\n",
    "    print(\"\\tShrank testing dataset to 50% of original size\")\n",
    "\n",
    "\n",
    "db = orca.OrcaDatabase(ORCA_DATABASE_NAME)  # creates and connects to an orca database instance\n",
    "print(\"Connected to Orca database\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll create a table to store the memories the model will use during training and inference. We add a text index to allow the model to search over them efficiently in its latent space.\n",
    "\n",
    "We use Orca's Huggingface Dataset Ingestor to load the data â€” `orcalib` provides ingestors for many other data sources as well!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory_labels = {\n",
    "    \"World News\": 0,\n",
    "    \"Sports\": 1,\n",
    "    \"Business\": 2,\n",
    "    \"Sci/Tech\": 3,\n",
    "    \"Swimming\": 4,\n",
    "}\n",
    "\n",
    "label_to_name = {v: k for k, v in memory_labels.items()}\n",
    "\n",
    "if RELOAD_MEMORY_TABLE:\n",
    "    db.drop_table(MEMORY_TABLE_NAME, error_if_not_exists=False)\n",
    "    train_table = db.create_table(\n",
    "        if_table_exists=TableCreateMode.REPLACE_CURR_TABLE,\n",
    "        table_name=MEMORY_TABLE_NAME,\n",
    "        text=orca.TextT.notnull,\n",
    "        label=orca.EnumT[memory_labels].notnull,\n",
    "    )\n",
    "    # We'll use this index to efficiently retrieve memories during training/inference.\n",
    "    db.create_text_index(index_name=MEMORY_INDEX_NAME, table_name=MEMORY_TABLE_NAME, column=\"text\")\n",
    "\n",
    "    orca.HFDatasetIngestor(db, table_name=MEMORY_TABLE_NAME, dataset=ds_test, replace=True).run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During training, each element in the training/testing datasets is typically accessed multiple times. To avoid repeating the memory lookups, we'll pre-cache the lookup results as new features in our datasets. During training/testing, we can inject the lookups directly into the model to dramatically reduce training time. Using cached lookups doesn't change the semantics of training; it just makes it faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from datasets import Dataset\n",
    "from orcalib import OrcaLookupCacheBuilder\n",
    "\n",
    "# The cacher will perform the memory lookups and add the results to the Dataset, which\n",
    "# will significantly speed up training and inference.\n",
    "lookup_cacher = OrcaLookupCacheBuilder( \n",
    "    db=db,\n",
    "    index_name=MEMORY_INDEX_NAME,\n",
    "    num_memories=NUM_MEMS,\n",
    "    embedding_col_name=\"embedded_text\", # Prompt's embedding\n",
    "    # memory_column_aliases map the memory-lookup column names to feature names that will be added to the \n",
    "    # Dataset. This is useful for aligning the inputs to your model's forward() method and preventing\n",
    "    # naming conflicts in the Dataset\n",
    "    memory_column_aliases={\"$embedding\": \"memory_embeddings\", \"label\": \"memory_labels\"},\n",
    "    drop_exact_match=True,\n",
    ")\n",
    "\n",
    "if DO_TRAIN:\n",
    "    if REBUILD_DATASET_LOOKUPS:\n",
    "        ds_train = lookup_cacher.add_lookups_to_hf_dataset(ds_train, \"text\")\n",
    "        print(\"Added lookups to training dataset\")\n",
    "        ds_test = lookup_cacher.add_lookups_to_hf_dataset(ds_test, \"text\")\n",
    "        print(\"Added lookups to testing dataset\")\n",
    "\n",
    "        ds_train.save_to_disk(TRAIN_DATASET_FILENAME)\n",
    "        ds_test.save_to_disk(TEST_DATASET_FILENAME)\n",
    "        print(\"Saved datasets to disk. Re-run with REBUILD_DATASET_LOOKUPS=False to use cached datasets.\")\n",
    "    else:\n",
    "        ds_train = Dataset.load_from_disk(TRAIN_DATASET_FILENAME)\n",
    "        print(\"Loaded training dataset from disk\")\n",
    "        ds_test = Dataset.load_from_disk(TEST_DATASET_FILENAME)\n",
    "        print(\"Loaded testing dataset from disk\")\n",
    "        \n",
    "    train_loader = DataLoader(ds_train, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    test_loader = DataLoader(ds_test, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    print(\"Created data loaders\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building and Training the Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Definition\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we're defining a simple classification model for the news headlines dataset using one of the pre-built, memory-augmented pytorch layers that are part of orcalib.\n",
    "\n",
    "One distinction worth highlighting here is that we are setting the output dimension of the model (i.e. number of classes) higher than the 4 classes the dataset actually has. In a conventional model this would be pretty silly, but with an Orca Augmented model you can add new classes (up to the model output dimension) purely through memory modifications later on, without additional training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleOrcaClassificationModel(OrcaModel, LabelColumnNameMixin):\n",
    "\n",
    "    def __init__(self, lookup_cacher: Optional[OrcaLookupCacheBuilder] = None):\n",
    "        super().__init__(database=db)\n",
    "\n",
    "        self.lookup_cacher = lookup_cacher\n",
    "        self.head = OrcaClassificationHead(\n",
    "            model_dim=768,\n",
    "            num_classes=10,  # actually only have 5 classes, but we're leaving room to grow\n",
    "            memory_index_name=MEMORY_INDEX_NAME,\n",
    "            label_column_name=\"label\",\n",
    "            num_memories=NUM_MEMS,\n",
    "            dropout=0.1,\n",
    "            activation=torch.nn.functional.relu,\n",
    "            num_layers=1,\n",
    "            classification_mode=ClassificationMode.MEMORY_BOUND,  # forces the model to predict from the memory distribution, leading to much better generalization to un-seen data (at the price of slightly lower static data accuracy)\n",
    "            drop_exact_match=DropExactMatchOption.TRAINING_ONLY,\n",
    "            exact_match_threshold=0.99,\n",
    "        )\n",
    "\n",
    "    def forward(self, x, memory_embeddings=None, memory_labels=None):\n",
    "        if self.lookup_cacher is not None:\n",
    "            self.lookup_cacher.inject_lookup_results(\n",
    "                self,\n",
    "                memory_embeddings=memory_embeddings,\n",
    "                memory_labels=memory_labels,\n",
    "            )\n",
    "        logits = self.head(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Utils\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Very vanilla training loop setup. Nothing particularly interesting or unique to Orca happening in this section.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Iterator\n",
    "\n",
    "\n",
    "@dataclass(slots=True)\n",
    "class PreparedRows:\n",
    "    text: List[str]\n",
    "    labels: torch.Tensor\n",
    "    memory_labels: torch.Tensor\n",
    "    memory_embeddings: torch.Tensor\n",
    "\n",
    "\n",
    "def prep_data(iterator) -> Iterator[PreparedRows]:\n",
    "    for stuff in iterator:\n",
    "        pass\n",
    "        inputs = torch.stack(stuff[\"embedded_text\"], dim=1).float().to(DEVICE)\n",
    "        labels = stuff[\"label\"].to(torch.int64).to(DEVICE)\n",
    "        memory_labels = torch.stack(stuff[\"memory_labels\"], dim=1).to(torch.int64).to(DEVICE)\n",
    "        memory_embeddings = torch.stack([\n",
    "            torch.stack(t, dim=1) for t in stuff[\"memory_embeddings\"]\n",
    "        ], dim=1).float().to(DEVICE)\n",
    "        yield PreparedRows(inputs, labels, memory_labels, memory_embeddings)\n",
    "\n",
    "\n",
    "def get_accuracy(logits, labels):\n",
    "    _, preds = torch.max(logits, 1)\n",
    "    return (preds == labels).float().mean().item()\n",
    "\n",
    "\n",
    "def get_test_accuracy(model, loader, progress_bar=False, live_lookups=False):\n",
    "    model.eval()\n",
    "    if progress_bar:\n",
    "        wrapper = tqdm\n",
    "    else:\n",
    "        wrapper = lambda x: x\n",
    "    with torch.no_grad():\n",
    "        test_acc = 0.0\n",
    "        test_steps = 0\n",
    "        for batch in prep_data(wrapper(loader)):\n",
    "            if live_lookups:\n",
    "                outputs = model(batch.text)\n",
    "            else:\n",
    "                outputs = model.forward(batch.text, memory_embeddings=batch.memory_embeddings, memory_labels=batch.memory_labels)\n",
    "            test_acc += get_accuracy(outputs, batch.labels)\n",
    "            test_steps += 1\n",
    "        avg_test_acc = test_acc / test_steps\n",
    "    model.train()\n",
    "    return avg_test_acc\n",
    "\n",
    "\n",
    "def train_one_epoch(model: torch.nn.Module, optimizer, epoch: int, verbosity: int = 0):\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    running_acc = 0.0\n",
    "    steps = 0\n",
    "    for batch in prep_data(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model.forward(batch.text, memory_embeddings=batch.memory_embeddings, memory_labels=batch.memory_labels)\n",
    "        loss = criterion(outputs, batch.labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        running_acc += get_accuracy(outputs, batch.labels)\n",
    "        steps += 1\n",
    "        if verbosity > 0 and steps % verbosity == 0:\n",
    "            avg_loss = running_loss / steps\n",
    "            avg_acc = running_acc / steps\n",
    "            print(f\"\\t Step {steps}, Loss: {avg_loss:.4f}, Accuracy: {avg_acc:.4f}\")\n",
    "\n",
    "    avg_loss = running_loss / steps\n",
    "    avg_acc = running_acc / steps\n",
    "    print(\n",
    "        f\"Epoch [{epoch+1}/{NUM_EPOCHS}], Loss: {avg_loss:.4f}, Accuracy: {avg_acc:.4f}, Test Accuracy: {get_test_accuracy(model, test_loader):.4f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleOrcaClassificationModel(lookup_cacher).to(DEVICE).to(DTYPE)\n",
    "embed_model = AutoModel.from_pretrained(\"sentence-transformers/multi-qa-mpnet-base-dot-v1\").to(DEVICE)\n",
    "embed_tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/multi-qa-mpnet-base-dot-v1\")\n",
    "\n",
    "if USE_PRETRAINED:\n",
    "    model.load_state_dict(torch.load(PRETRAINED_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DO_TRAIN:\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "        train_one_epoch(\n",
    "            model,\n",
    "            optimizer,\n",
    "            epoch,\n",
    "        )\n",
    "    if DO_SAVE:\n",
    "        torch.save(model.state_dict(), PRETRAINED_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spinning up a Testing UI\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spinning up a simple Gradio UI to test the model manually, with Orca Curate observability and editability enabled.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DO_START_UI:\n",
    "    model.update_curate_settings(\n",
    "        model_id=\"news_classification_demo\", model_version=\"0.0.1\", extra_tags=[\"from_test_ui\"], batch_size=1\n",
    "    )\n",
    "    model.enable_curate()\n",
    "\n",
    "    def predict(text: str) -> str:\n",
    "        embedding = embed_model(**embed_tokenizer(text, return_tensors=\"pt\").to(DEVICE)).pooler_output.detach()\n",
    "        outputs = model(embedding)\n",
    "        _, pred = torch.max(outputs, dim=0)\n",
    "        label_name = label_to_name[pred.item()]\n",
    "        model.record_model_input_output(text, label_name)\n",
    "        return label_name\n",
    "\n",
    "    with gr.Blocks() as demo:\n",
    "        with gr.Column():\n",
    "            with gr.Row():\n",
    "                inp = gr.Textbox(lines=5, placeholder=\"Enter your text here...\", label=\"News Headline\")\n",
    "                outp = gr.Label(num_top_classes=1)\n",
    "            btn = gr.Button(value=\"Submit\")\n",
    "            btn.click(fn=predict, inputs=inp, outputs=outp)\n",
    "\n",
    "    demo.launch(show_api=False, inline=False)\n",
    "\n",
    "    print(\"UI launched\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running a Benchmark\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the testset through the model with Orca observability enabled, incl. tracking model inputs, outputs, and a feedback score (how well did the model do). Enables finding and iterating on \"bad\" examples in the Orca UI easily.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_dataset(model, dataset):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_acc: float = 0.0\n",
    "        test_steps: int = 0\n",
    "        for item in tqdm(dataset):\n",
    "            expected_label = item[\"label\"]\n",
    "            expected_label_name = label_to_name[expected_label]\n",
    "            input_text = item[\"text\"]\n",
    "            input_embedding = embed_model(\n",
    "                **embed_tokenizer(input_text, return_tensors=\"pt\").to(DEVICE)\n",
    "            ).pooler_output.detach()\n",
    "            model.update_curate_settings(extra_metadata={\"expected_label\": f\"{expected_label_name} ({expected_label})\"})\n",
    "            model_outputs = model(input_embedding)\n",
    "            predicted_label: int = torch.max(model_outputs, dim=0)[1].item()\n",
    "            predicted_label_name = label_to_name[predicted_label]\n",
    "            correct = predicted_label == expected_label\n",
    "            test_acc += 1.0 if correct else 0.0\n",
    "            model.record_curate_scores(1.0 if correct else -1.0)\n",
    "            model.record_model_input_output(input_text, f\"{predicted_label_name} ({predicted_label})\")\n",
    "            test_steps += 1\n",
    "        avg_test_acc = test_acc / test_steps\n",
    "    model.train()\n",
    "    return avg_test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DO_RUN_BENCHMARK:\n",
    "    model.update_curate_settings(\n",
    "        model_id=\"news_classification_demo\", model_version=\"0.0.1\", extra_tags={\"testset_benchmark\"}, batch_size=1\n",
    "    )\n",
    "    model.head.enable_curate()\n",
    "    test_acc = eval_dataset(model, ds_test)\n",
    "    print(f\"Test Accuracy: {test_acc:.1%}\")  # 90%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nearest_neighbor_classifier(input_text):\n",
    "    tokens = embed_tokenizer(input_text, return_tensors=\"pt\", padding=\"max_length\", truncation=True)\n",
    "    with torch.no_grad():\n",
    "        embeddings = embed_model(**tokens.to(DEVICE)).pooler_output\n",
    "    query = embeddings.cpu().numpy().tolist()\n",
    "    neighbors = db.vector_scan_index(\"train_index\", query).select(\"label\").fetch(NUM_MEMS).to_tensor(\"label\")\n",
    "    return neighbors.mode(dim=1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DO_RUN_BENCHMARK:\n",
    "    correct = 0\n",
    "    steps = 0\n",
    "    for batch in tqdm(DataLoader(ds_test, batch_size=20)):\n",
    "        texts = batch[\"text\"]\n",
    "        expected_labels = batch[\"label\"]\n",
    "        predicted_labels = nearest_neighbor_classifier(texts)\n",
    "        correct += (expected_labels == predicted_labels).sum().item()\n",
    "        steps += 20\n",
    "    print(f\"KNN Ensemble Accuracy: {correct/steps:.1%}\")  # 91%"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "orca-research-42SkfAmm-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
